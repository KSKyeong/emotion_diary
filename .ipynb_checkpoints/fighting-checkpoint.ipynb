{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from PIL import Image\n",
    "from PIL.ExifTags import TAGS\n",
    "import folium \n",
    "import base64\n",
    "import datetime\n",
    "import webbrowser\n",
    "import pyautogui\n",
    "import glob\n",
    "\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "\n",
    "#--------#\n",
    "\n",
    "from utils.datasets import get_labels\n",
    "from utils.inference import detect_faces\n",
    "from utils.inference import draw_text\n",
    "from utils.inference import draw_bounding_box\n",
    "from utils.inference import apply_offsets\n",
    "from utils.inference import load_detection_model\n",
    "from utils.inference import load_image\n",
    "from utils.preprocessor import preprocess_input\n",
    "\n",
    "#\n",
    "\n",
    "from operator import eq\n",
    "\n",
    "\n",
    "def GPS_Marker(filename):\n",
    "    extension = filename.split('.')[-1]\n",
    "    if (extension == 'jpg') | (extension == 'JPG') | (extension == 'jpeg') | (extension == 'JPEG'):\n",
    "        try:\n",
    "            img = Image.open(filename)\n",
    "            info = img._getexif()\n",
    "            exif = {}\n",
    "            for tag, value in info.items():\n",
    "                decoded = TAGS.get(tag, tag)\n",
    "                exif[decoded] = value\n",
    "            # from the exif data, extract gps\n",
    "            exifGPS = exif['GPSInfo']\n",
    "            latData = exifGPS[2]\n",
    "            lonData = exifGPS[4]\n",
    "            # calculae the lat / long\n",
    "            latDeg = latData[0][0] / float(latData[0][1])\n",
    "            latMin = latData[1][0] / float(latData[1][1])\n",
    "            latSec = latData[2][0] / float(latData[2][1])\n",
    "            lonDeg = lonData[0][0] / float(lonData[0][1])\n",
    "            lonMin = lonData[1][0] / float(lonData[1][1])\n",
    "            lonSec = lonData[2][0] / float(lonData[2][1])\n",
    "            # correct the lat/lon based on N/E/W/S\n",
    "            Lat = (latDeg + (latMin + latSec / 60.0) / 60.0)\n",
    "            if exifGPS[1] == 'S': Lat = Lat * -1\n",
    "            Lon = (lonDeg + (lonMin + lonSec / 60.0) / 60.0)\n",
    "            if exifGPS[3] == 'W': Lon = Lon * -1\n",
    "            # print file\n",
    "            msg = \"There is GPS info in this picture located at \" + str(Lat) + \",\" + str(Lon)\n",
    "            print (msg)\n",
    "            \n",
    "        except:\n",
    "                print ('There is no GPS info in this picture')\n",
    "                pass\n",
    "        \n",
    "        pic = base64.b64encode(open(filename,'rb').read()).decode()\n",
    "        image_tag = '<img src=\"data:image/jpeg;base64,{}\"style=\"width:180px;height:200px;\">'.format(pic)\n",
    "        iframe = folium.IFrame(image_tag, width=150, height=200)\n",
    "        pop = folium.Popup(iframe, max_width=400)\n",
    "        name=filename.split('z')[0]\n",
    "        if name =='Happy' : \n",
    "            ic=\"star\"\n",
    "            col=\"pink\"\n",
    "        if name =='Sad' : col = \"black\"\n",
    "        if name =='Angry' : \n",
    "            #ic=\"star\"\n",
    "            col=\"cadetblue\"\n",
    "        if name =='Neutral' : \n",
    "            #ic=\"cloud\"\n",
    "            col=\"green\"\n",
    "        \n",
    "        cft=datetime.datetime.fromtimestamp(os.path.getctime(filename))\n",
    "        strcft=cft.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        print(strcft)\n",
    "        \n",
    "       \n",
    "        folium.Marker([str(Lat),str(Lon)],popup=pop,icon=folium.Icon(color=col),tooltip=name +'&'+ strcft).add_to(m)\n",
    "        m.save('testtest.html')\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "#카메라 실행 중에 아무 키나 입력 받으면 그 사진을 저장한다.\n",
    "def start(folder_to_save):\n",
    "    \n",
    "    cv2.destroyAllWindows() # 재촬영 시 모두 포맷하고 사진 찍을 준비.\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = capture.read() # 실시간으로 프레임을 받아준다\n",
    "        cv2.imshow(\"VideoFrame\", frame)\n",
    "        if cv2.waitKey(1) > 0: # 아무 키나 입력 할 때까지 실시간 카메라 보여줌\n",
    "            #키 입력 시 사진 바로 captured_img에 저장됨\n",
    "            \n",
    "            cv2.imwrite(os.path.join(folder_to_save , 'captured_img.jpg'), frame) # 변수 활용(시간, 위치)\n",
    "            return folder_to_save + '\\captured_img.jpg'\n",
    "            \n",
    "def image_handler(folder_to_save):  # 사진 확인 및 재촬영 의사를 확인한다.\n",
    "    #saved_path = \"\"\n",
    "    while(True):\n",
    "        saved_path = start(folder_to_save) # 아무 키나 눌렀을 때 저장되고, 그 사진의 저장된 경로를 반환 해준다.\n",
    "        img_file = r'C:\\Users\\kyung\\Anaconda3\\images\\captured_img.jpg'  # 변수 활용(시간, 위치)\n",
    "        img = cv2.imread(img_file,cv2.IMREAD_COLOR) \n",
    "        cv2.imshow('captured_image',img) \n",
    "    \n",
    "        key = 12354312345\n",
    "        key = cv2.waitKey(0) & 0xFF \n",
    "        if key == ord('r'): # r키 입력 시 재촬영\n",
    "            #start(path)\n",
    "            key = 1212312313\n",
    "            continue\n",
    "        elif key == ord('s'): # s 입력 시 저장\n",
    "            #cv2.imwrite('copy_images/img1_copy.jpg',img) \n",
    "            return saved_path;\n",
    "        #cv2.destroyAllWindow() # 권한이 없다고 한다.\n",
    "        \n",
    "def check_recoged_img(recoged_img_path): # recognition 함수 작동 후 저장된 얼굴 인식 결과 사진을 잠깐 확인 할 수 있게 해준다.\n",
    "    img = cv2.imread(recoged_img_path,cv2.IMREAD_COLOR) \n",
    "    while(True):\n",
    "        cv2.imshow('captured_image',img) \n",
    "\n",
    "        if cv2.waitKey(1) > 0:\n",
    "            break;\n",
    "\n",
    "\n",
    "            \n",
    "def recognition(f_2_s):\n",
    "    tt = False\n",
    "    #recap == False\n",
    "    # parameters for loading data and images\n",
    "    #image_path = path_r\n",
    "    image_path = image_handler(f_2_s)\n",
    "    detection_model_path = '../trained_models/detection_models/haarcascade_frontalface_default.xml'\n",
    "    emotion_model_path = '../trained_models/emotion_models/fer2013_mini_XCEPTION.102-0.66.hdf5'\n",
    "    emotion_labels = get_labels('fer2013')\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "    # hyper-parameters for bounding boxes shape\n",
    "    emotion_offsets = (20, 40)\n",
    "    emotion_offsets = (0, 0)\n",
    "\n",
    "    # loading models\n",
    "    face_detection = load_detection_model(detection_model_path)\n",
    "    emotion_classifier = load_model(emotion_model_path, compile=False)\n",
    "\n",
    "    # getting input model shapes for inference\n",
    "    emotion_target_size = emotion_classifier.input_shape[1:3]\n",
    "\n",
    "    # loading images\n",
    "    rgb_image = load_image(image_path, grayscale=False)\n",
    "    gray_image = load_image(image_path, grayscale=True)\n",
    "    gray_image = np.squeeze(gray_image)\n",
    "    gray_image = gray_image.astype('uint8')\n",
    "\n",
    "    faces = detect_faces(face_detection, gray_image)\n",
    "    for face_coordinates in faces:\n",
    "\n",
    "        x1, x2, y1, y2 = apply_offsets(face_coordinates, emotion_offsets)\n",
    "        gray_face = gray_image[y1:y2, x1:x2]\n",
    "\n",
    "        try:\n",
    "            gray_face = cv2.resize(gray_face, (emotion_target_size))\n",
    "        except:\n",
    "            #pyautogui.confirm(text='one more', title='test', buttons=['ok', 'exit'])\n",
    "            #recap = True\n",
    "            continue\n",
    "\n",
    "        gray_face = preprocess_input(gray_face, True)\n",
    "        gray_face = np.expand_dims(gray_face, 0)\n",
    "        gray_face = np.expand_dims(gray_face, -1)\n",
    "        emotion_label_arg = np.argmax(emotion_classifier.predict(gray_face))\n",
    "        emotion_text = \"\"\n",
    "        emotion_text = emotion_labels[emotion_label_arg]\n",
    "        \n",
    "        \n",
    "        \n",
    "        #감정인식이 성공되면 감정 상태를 물어보고, 감정 확인 후 저장 or 탈출\n",
    "        \n",
    "        \n",
    "        tof = pyautogui.confirm(text='Are you '+emotion_text+'?', title=emotion_text, buttons=['yes', 'no'])\n",
    "        if(tof == 'yes'):\n",
    "            tt = True\n",
    "            color = (255, 0, 0) # 감정 정보 글씨 빨간색, 사각형도\n",
    "\n",
    "            draw_bounding_box(face_coordinates, rgb_image, color)\n",
    "            draw_text(face_coordinates, rgb_image, emotion_text, color, 0, -30, 1.5, 2)\n",
    "        elif(tof == 'no'):\n",
    "            tt = False\n",
    "            break;\n",
    "        \n",
    "        #color = (255, 0, 0) # 감정 정보 글씨 빨간색, 사각형도\n",
    "\n",
    "        #draw_bounding_box(face_coordinates, rgb_image, color)\n",
    "        #draw_text(face_coordinates, rgb_image, emotion_text, color, 0, -30, 1.5, 2)\n",
    "    if(tt == True):\n",
    "        bgr_image = cv2.cvtColor(rgb_image, cv2.COLOR_RGB2BGR)\n",
    "        cv2.imwrite('../images/predicted_test_image.png', bgr_image) # 변수 활용\n",
    "        check_recoged_img('../images/predicted_test_image.png')\n",
    "    else:\n",
    "        pyautogui.alert(text='no emtion captured', title='error', button='OK')\n",
    "    \n",
    "    \n",
    "    # 저장된 사진을 감정 예측 후 predicted_test_image.jpg로 저장\n",
    "    \n",
    "#def to_text(path):\n",
    "    #return path+\".txt\"\n",
    "\n",
    "def message_to_recap():\n",
    "    a=pyautogui.confirm(text='one more', title='test', buttons=['ok', 'exit'])\n",
    "    return a;\n",
    "\n",
    "#def write_diary():\n",
    "    \n",
    "   \n",
    "\n",
    "            \n",
    "###################################################################################################################################\n",
    "\n",
    "if __name__ == '__main__': \n",
    "    capture = cv2.VideoCapture(0)\n",
    "    capture.set(cv2.CAP_PROP_FRAME_WIDTH, 900)\n",
    "    capture.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "    folder_to_save = r\"C:\\Users\\kyung\\Anaconda3\\images\"\n",
    "    #global recap = False\n",
    "    #recognition(image_handler(folder_to_save))\n",
    "    \n",
    "    recognition(folder_to_save)\n",
    "    \n",
    "    \n",
    "    capture.release() \n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    '''\n",
    "    m= folium.Map(location=[37.56,127],zoom_start=12.5)\n",
    "    m.save(r\"testtest.html\")\n",
    "    \n",
    "    GPS_Marker(r\"Happyz.jpg\")\n",
    "    GPS_Marker(r\"Happyz2.jpg\")\n",
    "    #GPS_Marker(r\"Angryz.jpg\")\n",
    "    GPS_Marker(r\"Neutralz.jpg\")\n",
    "    GPS_Marker(r\"Neutralz2.jpg\")\n",
    "    \n",
    "    \n",
    "    webbrowser.open(r\"testtest.html\")'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#####################################################################################################\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
